---
title: 'Imbalanced Regression: An Empirical Case Study'
author: "Sam Kolins, Atishay Sehgal, Deepika S. Namboothiri, Sarah Wu"
date: "December 5, 2018"
output: html_document
---

```{r setup, include=FALSE}
chooseCRANmirror(graphics=FALSE, ind=1)
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this project, we will be investigating the issue of imbalanced regression in the Google Analytics customer revenue prediction data set available [here](https://www.kaggle.com/c/ga-customer-revenue-prediction) on Kaggle. In this Kaggle competition, entrants are challenged to predict customer revenue from customers who purchase items at the online Google Merchandise Store. This data set contains a host of explanatory variables, ranging from total pageviews to the time of accessing the site to the location of the customer (down to city or region if available), all for the purposes of predicting customer revenue (the response variable), available in the training data only. This may seem like a fairly pedestrian regression problem at first glance, but there is an interesting wrinkle that must be addressed; as you will see in the data, over 98% of the customers with recorded data do not spend money on anything in the store.

The problem of **imbalanced _classification_** is well-known in data analysis. A naïve classifier using traditional methods will find that, in cases where the imbalance is especially strong (for example, a 90%-10% split between the majority and minority classes), it can just trivially assign every test point to the majority class and get a good accuracy score (in this case, with 90-10 split, the accuracy will be 90%, because every test point in the majority gets correctly predicted while every test point in the minority is incorrectly predicted). There are a host of ways to deal with imbalanced classification, some of which we tackle here.

But the problem of **imbalanced _regression_** is actually a much more open problem. It seems hard to envision what an imbalanced regression problem might look like as imbalanced data sets seem to naturally imply categorization into classes. However, in the case of the Google Merchandise data set above, we see that the vast majority of customers do not pay (we turned the `NA`'s into `0`'s by assumption), but the remaining customers pay a wide variety of differing amounts. In other words, we have a vast majority of zeroes and a small collection of non-zeroes that vary continuously.

There are a few techniques that can handle imbalanced regression data directly, such as XGBoost and **synthetic minority over-sampling technique (SMOTE) regression**, but we devised a different approach. Rather than try to solve the open problem of imbalanced regression, why don't we run a classification algorithm that handles imbalanced classification and then regress on the minority class values (in this case, the small subset of paid customers)? This could potentially be more powerful than simply running imbalanced regression directly, but it comes at the cost of potentially increased complexity. It also relies on more choices because we need to pick two different models: one for classification and one for regression. The data rows that correspond to a predicted classification label of "paid" are fed into the regresser, so depending on how effective the classifier is, the regresser could compound those errors, making MSE values potentially inflated or otherwise unreliable. As such, it will be important to report on both the metrics for the classifiers and the metrics for the regressers simultaneously. This method also requires the training data to be split into "subtraining" data and a validation set; this is largely for assessing the performance of the classifier, but could also be used to assess the performance of the regresser too (as the original test data lacks publicly available revenue values, we cannot evaluate classification metrics on it). Once we have obtained a final regression model post-classification, we can then run *that* on the original test data, achieving a set of predictions for that data.

All this being said, this project will be about comparing different **two-stage models**, as we are calling them here, that classify paid vs. non-paid customers and then run regression on the paid customers. The nice thing about two-stage models is that their pieces are customizable; any form of regression can be ran on any classifier, because the only inputs needed are whichever rows are preordained by the classifier as belonging to the "paid" subset. This makes them relatively easy to compare, as we can see which regresser works best with each classifier (and from there, determine the overall best model for this data set from all of our choices).

For classification, we are using the following models:

For regression, we are using the following models:

Ideally, the end result of this project will be an "MSE/metrics table" of sorts where each row represents a classifier and each column represents a regresser (or vice versa). This will give us an easy visual indication of which model appears to work best for the data. For now, though, we move on to the challenges of preprocessing this particular data set.

## Preprocessing

Install required packages
```{r load libraries, warning=FALSE, message=FALSE, echo=FALSE}

packages.used=c("glmnet", "ggplot2", "tidyverse", "pscl", "UBL", "ggraph", "igraph", "MASS", "lubridate", "e1071", "gridExtra")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

library(glmnet)
library(ggplot2)
library(tidyverse)
library(pscl)
library(UBL)
library(ggraph)
library(igraph)
library(MASS)
library(lubridate)
library(e1071)
library(gridExtra)
```


We reduce the train set to some chosen variables
```{r}
# Preprocessing
dat <- train %>% 
  transmute(revenue = ifelse(is.na(log(totals.transactionRevenue)) == T, 0, log(totals.transactionRevenue)),
            is.paid = factor(ifelse(revenue > 0, 1, 0)),
            browser = factor(case_when(device.browser == "Chrome" ~ "chrome",
                                device.browser %in% c("Safari", "Safari (in-app)") ~ "safari",
                                device.browser == "Firefox" ~ "firefox",
                                device.browser %in% c("Edge", "Internet Explorer") ~ "edge",
                                device.browser %in% c("Opera", "Opera Mini") ~ "opera",
                                TRUE ~ "other")),
            os = factor(case_when(device.operatingSystem == "Windows" ~ "windows",
                           device.operatingSystem == "Mackintosh" ~ "mackintosh",
                           device.operatingSystem == "Android" ~ "android",
                           device.operatingSystem == "iOS" ~ "ios",
                           device.operatingSystem  == "Linux" ~ "linux",
                           device.operatingSystem  == "ChromeOS" ~ "chromeos",
                           is.na(device.operatingSystem) == T ~ "unknown",
                           TRUE ~ "other")),
            device = factor(device.deviceCategory),
            continent = factor(ifelse(is.na(geoNetwork.continent) == T, "unknown", geoNetwork.continent)),
            pageviews = ifelse(is.na(totals.pageviews) == T, 0, totals.pageviews),
            bounces = ifelse(is.na(totals.bounces) == T, 0, totals.bounces),
            medium = factor(ifelse(is.na(trafficSource.medium) == T, "unknown", trafficSource.medium)),
            campaign = factor(ifelse(is.na(trafficSource.campaign) == T, 0, 1)),
            #keyword = trafficSource.keyword,
            channel = factor(channelGrouping),
            #visitor = fullVisitorId,
            source = factor(case_when(trafficSource.source %in% train[grep("google", trafficSource.source),]$trafficSource.source ~ "google",
                               trafficSource.source %in% train[grep("(direct)", trafficSource.source),]$trafficSource.source ~ "direct",
                               trafficSource.source %in% train[grep("facebook", trafficSource.source),]$trafficSource.source ~ "facebook",
                               trafficSource.source %in% train[grep("android", trafficSource.source),]$trafficSource.source ~ "android",
                               trafficSource.source %in% train[grep("bing", trafficSource.source),]$trafficSource.source ~ "bing",
                               trafficSource.source %in% train[grep("baidu", trafficSource.source),]$trafficSource.source ~ "baidu",
                               trafficSource.source %in% train[grep("yahoo", trafficSource.source),]$trafficSource.source ~ "yahoo",
                               trafficSource.source %in% train[grep("Partners", trafficSource.source),]$trafficSource.source ~ "partners",
                               trafficSource.source %in% train[grep("ask", trafficSource.source),]$trafficSource.source ~ "ask",
                               TRUE ~ "other")),
            hour = hour(as_datetime(train$visitStartTime)),
            month = month(as_datetime(train$visitStartTime)),
            day = factor(weekdays(as_datetime(train$visitStartTime))))


save(dat, file = "./data/cleaned.Rdata")

# Paying Customers
paid <- dat %>% filter(revenue > 0)

# Non-paying Customers
free <- dat %>% filter(revenue == 0)

```

##EDA
```{r}
# Average Pageviews per user for each device (Paid vs Free)

aggregate(pageviews~device, data = paid, mean)
aggregate(pageviews~device, data = free, mean)

a <- ggplot(aggregate(pageviews~device, data = paid, mean), aes(device, pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per device (Paid)")
b <- ggplot(aggregate(pageviews~device, data = free, mean), aes(device, pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per device (Free)")
grid.arrange(a,b, ncol = 2)


# Busiest month of the year (Paid vs Free)

aggregate(pageviews~month, data = paid, mean) # August: busiest; May: least busy
aggregate(pageviews~month, data = free, mean) # August: busiest; Nov: least busy

a <- ggplot(aggregate(pageviews~month, data = paid, mean), aes(factor(month), pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per month (Paid)")
b <- ggplot(aggregate(pageviews~month, data = free, mean), aes(factor(month), pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per month (Free)")
grid.arrange(a,b, ncol = 2)

# Busiest weekday (Paid vs Free)

aggregate(pageviews~day, data = paid, mean) 
aggregate(pageviews~day, data = free, mean) 

a <- ggplot(aggregate(pageviews~day, data = paid, mean), aes(day, pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per day of the week (Paid)")
b <- ggplot(aggregate(pageviews~day, data = free, mean), aes(day, pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per day of the week (Free)")
grid.arrange(a,b, ncol = 2)


# Busiest Hour (Paid vs Free)

aggregate(pageviews~hour, data = paid, mean) 
aggregate(pageviews~hour, data = free, mean) 

a <- ggplot(aggregate(pageviews~hour, data = paid, mean), aes(factor(hour), pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per hour of day (Paid)")
b <- ggplot(aggregate(pageviews~hour, data = free, mean), aes(factor(hour), pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per hour of day (Free)")
grid.arrange(a,b, ncol = 2)


# Median revenue earned per device

aggregate(revenue~device, data = paid, median)

# Median revenue earned per month

aggregate(revenue~month, data = paid, median)

# Median revenue earned per day of the week

aggregate(revenue~day, data = paid, median)

# Median revenue earned per hour of day

aggregate(revenue~hour, data = paid, median)


# Plots

# Looking at the distribution of hits by channel for paying customers vs free customers

ggplot(dat, aes(channel,pageviews,fill = channel)) + 
  geom_bar(stat = "identity") +
  facet_wrap(~ifelse(!is.na(revenue) == T, "Paid", "Free"), scales = "free") +
  ggtitle("Pageviews by channel") +
  coord_flip() +
  theme(axis.text.x=element_text(angle = -45, hjust = 0))

# Looking at the distribution of hits by device for paying customers vs free customers

ggplot(dat, aes(device,pageviews,fill = device)) + 
  geom_bar(stat = "identity") +
  facet_wrap(~ifelse(!is.na(revenue) == T, "Paid", "Free"), scales = "free") +
  ggtitle("Pageviews by Device") +
  coord_flip() +
  theme(axis.text.x=element_text(angle = -45, hjust = 0))
```


## One-Stage LASSO Regression

```{r}
# loading preprocessed data
load("./output/cleaned.Rdata")
# saved as "dat"
# revenue is log revenue of the original data set

# creating a test set from "dat" (which is all training data of the original data set)
set.seed(100)
samp <- sample(nrow(dat), 0.7 * nrow(dat), replace = FALSE)
train <- dat[samp, ]
test <- dat[-samp, ]
summary(train)
summary(test)

# creating model matrix from training data
train.mat <- model.matrix(revenue ~ ., data = train)

# creating y variable
logrev.train <- train$revenue
```

Running LASSO regression with the optimal lambda
```{r}
# running ElasticNet/LASSO regression (alpha = 1)
fit1 <- cv.glmnet(train.mat, logrev.train, alpha = 1)
fit1
plot(fit1)
opt.lambda <- fit1$lambda.min

# running LASSO with optimal lambda
fit1.opt <- glmnet(train.mat[, -1], logrev.train, family = "gaussian", alpha = 1, lambda = opt.lambda)
plot(glmnet(train.mat[, -1], logrev.train, family = "gaussian", alpha = 1), xvar = "lambda", label = TRUE)
# here are the coefficients. many of them have been shrunk to zero, but almost all of them are
# low anyway (< 10^-1, many < 10^-2)
coeffs <- fit1.opt$beta
coeffs
plot(coef(fit1.opt, s = opt.lambda)) # coeff plot
# only 17% of the deviance is explained by this model
fit1.opt$dev.ratio
```

The MSE for both training and test set appear very low
```{r}
pred1.train <- predict(fit1.opt, train.mat[, -1], s = opt.lambda)
sqrt(mean(logrev.train - pred1.train)^2) 
# training error (root MSE) is extremely low!
# overfitting is likely occurring

test.mat <- model.matrix(revenue ~ ., data = test)
pred1.test <- predict(fit1.opt, s = opt.lambda, newx = test.mat[, -1], type = "response")
logrev.test <- test$revenue
sqrt(mean(logrev.test - pred1.test)^2) # test error (root MSE) really low???
```


```{r}
# let's visually see if the test predictions are actually good
# the following is a matrix that stores the ID's of the data points, the true log revenue,
# and the predicted log revenue
test.compare <- cbind(logrev.test, pred1.test)
indices <- rownames(test.compare)
test.compare <- cbind(as.numeric(indices), test.compare)
colnames(test.compare) <- c("ID", "True", "Predicted")
rownames(test.compare) <- NULL
head(test.compare)

ggplot(data = as.data.frame(test.compare)) + 
  geom_point(mapping = aes(x = ID, y = Predicted, color = "Predicted"), alpha = 0.3) +
  geom_point(mapping = aes(x = ID, y = True, color = "True"), alpha = 0.3) +
  ylab("Log Revenue") + scale_color_manual("Legend", 
                                           breaks = c("Predicted", "True"),
                                           values = c("red", "blue"))
```


## One-Stage XGBoost Regression

```{r}

```


## Two-Stage Models



## Conclusion