---
title: 'Imbalanced Regression: An Empirical Case Study'
author: "Sam Kolins, Atishay Sehgal, Deepika S. Namboothiri, Sarah Wu"
date: "December 5, 2018"
output: html_document
---

```{r setup, include=FALSE}
chooseCRANmirror(graphics=FALSE, ind=1)
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this project, we will be investigating the issue of imbalanced regression in the Google Analytics customer revenue prediction data set available [here](https://www.kaggle.com/c/ga-customer-revenue-prediction) on Kaggle. In this Kaggle competition, entrants are challenged to predict customer revenue from customers who purchase items at the online Google Merchandise Store. This data set contains a host of explanatory variables, ranging from total pageviews to the time of accessing the site to the location of the customer (down to city or region if available), all for the purposes of predicting customer revenue (the response variable), available in the training data only. This may seem like a fairly pedestrian regression problem at first glance, but there is an interesting wrinkle that must be addressed; as you will see in the data, over 98% of the customers with recorded data do not spend money on anything in the store.

The problem of **imbalanced _classification_** is well-known in data analysis. A naïve classifier using traditional methods will find that, in cases where the imbalance is especially strong (for example, a 90%-10% split between the majority and minority classes), it can just trivially assign every test point to the majority class and get a good accuracy score (in this case, with 90-10 split, the accuracy will be 90%, because every test point in the majority gets correctly predicted while every test point in the minority is incorrectly predicted). There are a host of ways to deal with imbalanced classification, some of which we tackle here.

But the problem of **imbalanced _regression_** is actually a much more open problem. It seems hard to envision what an imbalanced regression problem might look like as imbalanced data sets seem to naturally imply categorization into classes. However, in the case of the Google Merchandise data set above, we see that the vast majority of customers do not pay (we turned the `NA`'s into `0`'s by assumption), but the remaining customers pay a wide variety of differing amounts. In other words, we have a vast majority of zeroes and a small collection of non-zeroes that vary continuously.

There are a few techniques that can handle imbalanced regression data directly, such as XGBoost and **synthetic minority over-sampling technique (SMOTE) regression**, but we devised a different approach. Rather than try to solve the open problem of imbalanced regression, why don't we run a classification algorithm that handles imbalanced classification and then regress on the minority class values (in this case, the small subset of paid customers)? This could potentially be more powerful than simply running imbalanced regression directly, but it comes at the cost of potentially increased complexity. It also relies on more choices because we need to pick two different models: one for classification and one for regression. The data rows that correspond to a predicted classification label of "paid" are fed into the regresser, so depending on how effective the classifier is, the regresser could compound those errors, making MSE values potentially inflated or otherwise unreliable. As such, it will be important to report on both the metrics for the classifiers and the metrics for the regressers simultaneously. This method also requires the training data to be split into "subtraining" data and a validation set; this is largely for assessing the performance of the classifier, but could also be used to assess the performance of the regresser too (as the original test data lacks publicly available revenue values, we cannot evaluate classification metrics on it). Once we have obtained a final regression model post-classification, we can then run *that* on the original test data, achieving a set of predictions for that data.

All this being said, this project will be about comparing different **two-stage models**, as we are calling them here, that classify paid vs. non-paid customers and then run regression on the paid customers. The nice thing about two-stage models is that their pieces are customizable; any form of regression can be ran on any classifier, because the only inputs needed are whichever rows are preordained by the classifier as belonging to the "paid" subset. This makes them relatively easy to compare, as we can see which regresser works best with each classifier (and from there, determine the overall best model for this data set from all of our choices).

For classification, we are using the following models:

For regression, we are using the following models:

Ideally, the end result of this project will be an "MSE/metrics table" of sorts where each row represents a classifier and each column represents a regresser (or vice versa). This will give us an easy visual indication of which model appears to work best for the data. For now, though, we move on to the challenges of preprocessing this particular data set.

## Preprocessing

Install required packages
```{r load libraries, warning=FALSE, message=FALSE, echo=FALSE}

#setwd("../Fall2018-project5-sec2proj5-grp3/")

packages.used=c("glmnet", "ggplot2", "tidyverse", "pscl", "UBL", "ggraph", "igraph", "MASS", "lubridate", "e1071", "gridExtra", "penalizedLDA", "psych", "irr", "pROC", "caret", "zoo", "DMwR", "ROSE", "xgboost", "earth")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

library(glmnet)
library(ggplot2)
library(tidyverse)
library(pscl)
library(UBL)
library(ggraph)
library(igraph)
library(MASS)
library(lubridate)
library(e1071)
library(gridExtra)
library(penalizedLDA)
library(psych)
library(irr)
library(pROC)
library(caret)
library(zoo)
library(DMwR)
library(ROSE)
library(xgboost)
library(earth)

```


We reduce the train set to some key chosen variables
```{r}
# Preprocessing
#save(train, file = "./data/train.Rdata")
#save(test, file = "./data/test.Rdata")

dat <- train %>% 
  transmute(revenue = ifelse(is.na(log(totals.transactionRevenue)) == T, 0, log(totals.transactionRevenue)),
            browser = factor(case_when(device.browser == "Chrome" ~ "chrome",
                                device.browser %in% c("Safari", "Safari (in-app)") ~ "safari",
                                device.browser == "Firefox" ~ "firefox",
                                device.browser %in% c("Edge", "Internet Explorer") ~ "edge",
                                device.browser %in% c("Opera", "Opera Mini") ~ "opera",
                                TRUE ~ "other")),
            os = factor(case_when(device.operatingSystem == "Windows" ~ "windows",
                           device.operatingSystem == "Mackintosh" ~ "mackintosh",
                           device.operatingSystem == "Android" ~ "android",
                           device.operatingSystem == "iOS" ~ "ios",
                           device.operatingSystem  == "Linux" ~ "linux",
                           device.operatingSystem  == "ChromeOS" ~ "chromeos",
                           is.na(device.operatingSystem) == T ~ "unknown",
                           TRUE ~ "other")),
            device = factor(device.deviceCategory),
            continent = factor(ifelse(is.na(geoNetwork.continent) == T, "unknown", geoNetwork.continent)),
            pageviews = ifelse(is.na(totals.pageviews) == T, 0, totals.pageviews),
            bounces = ifelse(is.na(totals.bounces) == T, 0, totals.bounces),
            medium = factor(ifelse(is.na(trafficSource.medium) == T, "unknown", trafficSource.medium)),
            campaign = factor(ifelse(is.na(trafficSource.campaign) == T, 0, 1)),
            #keyword = trafficSource.keyword,
            channel = factor(channelGrouping),
            #visitor = fullVisitorId,
            source = factor(case_when(trafficSource.source %in% train[grep("google", trafficSource.source),]$trafficSource.source ~ "google",
                               trafficSource.source %in% train[grep("(direct)", trafficSource.source),]$trafficSource.source ~ "direct",
                               trafficSource.source %in% train[grep("facebook", trafficSource.source),]$trafficSource.source ~ "facebook",
                               trafficSource.source %in% train[grep("android", trafficSource.source),]$trafficSource.source ~ "android",
                               trafficSource.source %in% train[grep("bing", trafficSource.source),]$trafficSource.source ~ "bing",
                               trafficSource.source %in% train[grep("baidu", trafficSource.source),]$trafficSource.source ~ "baidu",
                               trafficSource.source %in% train[grep("yahoo", trafficSource.source),]$trafficSource.source ~ "yahoo",
                               trafficSource.source %in% train[grep("Partners", trafficSource.source),]$trafficSource.source ~ "partners",
                               trafficSource.source %in% train[grep("ask", trafficSource.source),]$trafficSource.source ~ "ask",
                               TRUE ~ "other")),
            hour = hour(as_datetime(train$visitStartTime)),
            month = month(as_datetime(train$visitStartTime)),
            day = factor(weekdays(as_datetime(train$visitStartTime))))


newdat <- train %>% 
  transmute(revenue = ifelse(is.na(log(totals.transactionRevenue)) == T, 0, log(totals.transactionRevenue)),
            is.paid = factor(ifelse(revenue > 0, 1, 0)),
            browser = factor(case_when(device.browser == "Chrome" ~ "chrome",
                                       device.browser %in% c("Safari", "Safari (in-app)") ~ "safari",
                                       device.browser == "Firefox" ~ "firefox",
                                       device.browser %in% c("Edge", "Internet Explorer") ~ "edge",
                                       device.browser %in% c("Opera", "Opera Mini") ~ "opera",
                                       TRUE ~ "other")),
            os = factor(case_when(device.operatingSystem == "Windows" ~ "windows",
                                  device.operatingSystem == "Mackintosh" ~ "mackintosh",
                                  device.operatingSystem == "Android" ~ "android",
                                  device.operatingSystem == "iOS" ~ "ios",
                                  device.operatingSystem  == "Linux" ~ "linux",
                                  device.operatingSystem  == "ChromeOS" ~ "chromeos",
                                  is.na(device.operatingSystem) == T ~ "unknown",
                                  TRUE ~ "other")),
            device = factor(device.deviceCategory),
            continent = factor(ifelse(is.na(geoNetwork.continent) == T, "unknown", geoNetwork.continent)),
            pageviews = ifelse(is.na(totals.pageviews) == T, 0, totals.pageviews),
            bounces = ifelse(is.na(totals.bounces) == T, 0, totals.bounces),
            medium = factor(ifelse(is.na(trafficSource.medium) == T, "unknown", trafficSource.medium)),
            campaign = factor(ifelse(is.na(trafficSource.campaign) == T, 0, 1)),
            #keyword = trafficSource.keyword,
            channel = factor(channelGrouping),
            #visitor = fullVisitorId,
            source = factor(case_when(trafficSource.source %in% train[grep("google", trafficSource.source),]$trafficSource.source ~ "google",
                                      trafficSource.source %in% train[grep("(direct)", trafficSource.source),]$trafficSource.source ~ "direct",
                                      trafficSource.source %in% train[grep("facebook", trafficSource.source),]$trafficSource.source ~ "facebook",
                                      trafficSource.source %in% train[grep("android", trafficSource.source),]$trafficSource.source ~ "android",
                                      trafficSource.source %in% train[grep("bing", trafficSource.source),]$trafficSource.source ~ "bing",
                                      trafficSource.source %in% train[grep("baidu", trafficSource.source),]$trafficSource.source ~ "baidu",
                                      trafficSource.source %in% train[grep("yahoo", trafficSource.source),]$trafficSource.source ~ "yahoo",
                                      trafficSource.source %in% train[grep("Partners", trafficSource.source),]$trafficSource.source ~ "partners",
                                      trafficSource.source %in% train[grep("ask", trafficSource.source),]$trafficSource.source ~ "ask",
                                      TRUE ~ "other")),
            hour = hour(as_datetime(train$visitStartTime)),
            month = month(as_datetime(train$visitStartTime)),
            day = factor(weekdays(as_datetime(train$visitStartTime))))

#save(dat, file = "./data/cleaned.Rdata")
#save(newdat, file = "./data/oldcleaned.Rdata")

# Paying Customers
paid <- dat %>% filter(revenue > 0)

# Non-paying Customers
free <- dat %>% filter(revenue == 0)


```

##EDA
```{r}
# Average Pageviews per user for each device (Paid vs Free)

aggregate(pageviews~device, data = paid, mean)
aggregate(pageviews~device, data = free, mean)

a <- ggplot(aggregate(pageviews~device, data = paid, mean), aes(device, pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per device (Paid)")
b <- ggplot(aggregate(pageviews~device, data = free, mean), aes(device, pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per device (Free)")
grid.arrange(a,b, ncol = 2)


# Busiest month of the year (Paid vs Free)

aggregate(pageviews~month, data = paid, mean) # August: busiest; May: least busy
aggregate(pageviews~month, data = free, mean) # August: busiest; Nov: least busy

a <- ggplot(aggregate(pageviews~month, data = paid, mean), aes(factor(month), pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per month (Paid)")
b <- ggplot(aggregate(pageviews~month, data = free, mean), aes(factor(month), pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per month (Free)")
grid.arrange(a,b, ncol = 2)

# Busiest weekday (Paid vs Free)

aggregate(pageviews~day, data = paid, mean) 
aggregate(pageviews~day, data = free, mean) 

a <- ggplot(aggregate(pageviews~day, data = paid, mean), aes(day, pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per day of the week (Paid)")
b <- ggplot(aggregate(pageviews~day, data = free, mean), aes(day, pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per day of the week (Free)")
grid.arrange(a,b, ncol = 2)


# Busiest Hour (Paid vs Free)

aggregate(pageviews~hour, data = paid, mean) 
aggregate(pageviews~hour, data = free, mean) 

a <- ggplot(aggregate(pageviews~hour, data = paid, mean), aes(factor(hour), pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per hour of day (Paid)")
b <- ggplot(aggregate(pageviews~hour, data = free, mean), aes(factor(hour), pageviews)) +
  geom_bar(stat = "identity") + labs(title = "Pageviews per user per hour of day (Free)")
grid.arrange(a,b, ncol = 2)


# Median revenue earned per device

aggregate(revenue~device, data = paid, median)

# Median revenue earned per month

aggregate(revenue~month, data = paid, median)

# Median revenue earned per day of the week

aggregate(revenue~day, data = paid, median)

# Median revenue earned per hour of day

aggregate(revenue~hour, data = paid, median)


# Plots

# Looking at the distribution of hits by channel for paying customers vs free customers

ggplot(dat, aes(channel,pageviews,fill = channel)) + 
  geom_bar(stat = "identity") +
  facet_wrap(~ifelse(!is.na(revenue) == T, "Paid", "Free"), scales = "free") +
  ggtitle("Pageviews by channel") +
  coord_flip() +
  theme(axis.text.x=element_text(angle = -45, hjust = 0))

# Looking at the distribution of hits by device for paying customers vs free customers

ggplot(dat, aes(device,pageviews,fill = device)) + 
  geom_bar(stat = "identity") +
  facet_wrap(~ifelse(!is.na(revenue) == T, "Paid", "Free"), scales = "free") +
  ggtitle("Pageviews by Device") +
  coord_flip() +
  theme(axis.text.x=element_text(angle = -45, hjust = 0))


```

##One Step Models

## One-Stage LASSO Regression

```{r}
# loading preprocessed data
load("./output/cleaned.Rdata")
# saved as "dat"
# revenue is log revenue of the original data set

# creating a test set from "dat" (which is all training data of the original data set)
set.seed(100)
samp <- sample(nrow(dat), 0.7 * nrow(dat), replace = FALSE)
train <- dat[samp, ]
test <- dat[-samp, ]
summary(train)
summary(test)

# creating model matrix from training data
train.mat <- model.matrix(revenue ~ ., data = train)

# creating y variable
logrev.train <- train$revenue

# running ElasticNet/LASSO regression (alpha = 1)
fit1 <- cv.glmnet(train.mat, logrev.train, alpha = 1)
fit1
plot(fit1)
opt.lambda <- fit1$lambda.min

# running LASSO with optimal lambda
fit1.opt <- glmnet(train.mat[, -1], logrev.train, family = "gaussian", alpha = 1, lambda = opt.lambda)
plot(glmnet(train.mat[, -1], logrev.train, family = "gaussian", alpha = 1), xvar = "lambda", label = TRUE)
# here are the coefficients. many of them have been shrunk to zero, but almost all of them are
# low anyway (< 10^-1, many < 10^-2)
coeffs <- fit1.opt$beta
coeffs
plot(coef(fit1.opt, s = opt.lambda)) # coeff plot
# only 17% of the deviance is explained by this model
fit1.opt$dev.ratio

pred1.train <- predict(fit1.opt, train.mat[, -1], s = opt.lambda)
sqrt(mean((logrev.train - pred1.train)^2)) # training error = 1.8153

test.mat <- model.matrix(revenue ~ ., data = test)
pred1.test <- predict(fit1.opt, s = opt.lambda, newx = test.mat[, -1], type = "response")
logrev.test <- test$revenue
sqrt(mean((logrev.test - pred1.test)^2)) # test error = 1.8298

# let's visually see if the test predictions are actually good
# the following is a matrix that stores the ID's of the data points, the true log revenue,
# and the predicted log revenue
test.compare <- cbind(logrev.test, pred1.test)
indices <- rownames(test.compare)
test.compare <- cbind(as.numeric(indices), test.compare)
colnames(test.compare) <- c("ID", "True", "Predicted")
rownames(test.compare) <- NULL
head(test.compare)

ggplot(data = as.data.frame(test.compare)) + 
  geom_point(mapping = aes(x = ID, y = Predicted, color = "Predicted"), alpha = 0.3) +
  geom_point(mapping = aes(x = ID, y = True, color = "True"), alpha = 0.3) +
  ylab("Log Revenue") + scale_color_manual("Legend", 
                                           breaks = c("Predicted", "True"),
                                           values = c("red", "blue"))

```


## One-Stage XGBoost Regression

We attempt 2 models for xgboost regression: one based on the linear objective function and another based on the Tweedie Distribution objective function.

```{r}
## This is for the imbalanced regression problem. "One Step Model"

# XgBoost (Linear Regression followed by Tweedie Regression)
load("./data/cleaned.Rdata")

# Splitting the data into training and test
set.seed(100)
samp <- sample(nrow(dat), 0.7*nrow(dat), replace = FALSE)
train <- dat[samp,]
test <- dat[-samp,]
summary(train)
summary(test)

# Model 1: Xgboost (Linear Objective function)

mod <- model.matrix(revenue ~ ., data = train)
modte <- model.matrix(revenue ~ ., data = test)

# Parameters chosen after cross validation and hyperparameter tuning
xgb <- xgboost(data = mod, 
               label = train$revenue, 
               eta = 0.1,
               max_depth = 8, 
               nrounds = 100,
               eval_metric = "rmse",
               objective = "reg:linear"
)

# Predicting for the test set
y_pred <- predict(xgb, newdata = modte)

# RMSE
xgb.rmse <- sqrt(mean((y_pred-test$revenue)^2)); xgb.rmse
#1.70

# Model 2: Xgboost (Tweedie Distribution Objective function)

# Parameters chosen after cross validation and hyperparameter tuning (eta, tweedie_variance_power, nrounds)
xgb.tweedie <- xgboost(data = mod, 
               label = train$revenue, 
               eta = 0.1,
               max_depth = 8, 
               nrounds = 100,
               eval_metric = "rmse",
               objective = "reg:tweedie",
               tweedie_variance_power = 1.1
)

# Predicting for the test set
y_pred.tweedie <- predict(xgb.tweedie, newdata = modte)

# RMSE
tweedie.rmse <- sqrt(mean((y_pred.tweedie-test$revenue)^2)); tweedie.rmse
#1.68

```
At 1.68, the RMSE for the Tweedie distribution is slightly lower than linear regression.


Additionally, the team attempted classification by penalized logistic regression, however, the model did not converge for the ideal lambda, and this was ultimately unused for our two step models. The code can be found in the 'doc' folder.


## Two-Stage Models

#Two Step Model: Penalized LDA + LASSO /Ridge

Penalized LDA puts a penalty on classification mistakes made on the minority class. We use LASSO and Ridge to strengthen the results of our prediction and avoid overfitting.
```{r}
# for classification performance metrics, see penalized_LDA_classification_skolins.R

# loading preprocessed data
load("./output/cleaned.Rdata")
# saved as "dat"
# revenue is log revenue of the original data set

# creating a test set from "dat" (which is all training data of the original data set)
set.seed(100)
samp <- sample(nrow(dat), 0.7 * nrow(dat), replace = FALSE)
train <- dat[samp, ]
test <- dat[-samp, ]

# creating classes for the data; 2 = paid, 1 = not paid (revenue = 0)
# the PenalizedLDA function prefers class labels that are (1, 2, ..., N), not (0, 1, ..., N)
paid.train <- ifelse(train$revenue > 0, 2, 1)

# creating model matrices
mdat <- model.matrix(revenue ~ ., data = dat)
train.mat <- mdat[samp, ]
test.mat <- mdat[-samp, ]

# running penalized LDA with standard LASSO/L1 penalties
penlda <- PenalizedLDA(train.mat[, -1], paid.train, xte = test.mat[, -1], lambda = 0.14, K = 1)

# true classes for test set
paid.test <- ifelse(test$revenue > 0, 2, 1)

# now time to feed the predicted paid customers into the regressers...
# isolating the predicted paid customers (PPC's)
index.ppc <- penlda$ypred == 2
index.ppc <- as.vector(index.ppc)
ppc <- test[index.ppc, ]
# this is now our test set for the regressers

# creating model matrix for the regressers
ppc.mat <- model.matrix(revenue ~ ., data = ppc)

# creating a test set from "ppc"
set.seed(100)
samp2 <- sample(nrow(ppc), 0.85 * nrow(ppc), replace = FALSE)
ppc.train <- ppc[samp2, ]
ppc.test <- ppc[-samp2, ]
summary(ppc.train)
summary(ppc.test)

# creating model matrices for the subsets above
ppctrain.mat <- ppc.mat[samp2, ]
ppctest.mat <- ppc.mat[-samp2, ]

# start with LASSO first
# linear LASSO cross-validation
cv.lasso <- cv.glmnet(ppctrain.mat[, -1], ppc.train$revenue, alpha = 1)
plot(cv.lasso)
lam.lasso <- cv.lasso$lambda.min
# LASSO regression
fit.lasso <- glmnet(ppctrain.mat[, -1], ppc.train$revenue, alpha = 1, lambda = lam.lasso)
fit.lasso$beta # coefficients
# training error
pred.lassotrain <- predict(fit.lasso, ppctrain.mat[, -1], s = lam.lasso) 
sqrt(mean((ppc.train$revenue - pred.lassotrain)^2))
# test error
pred.lassotest <- predict(fit.lasso, ppctest.mat[, -1], s = lam.lasso)
sqrt(mean((ppc.test$revenue - pred.lassotest)^2))
# these errors are MUCH worse than just running the regressers on all of the paid data
# that goes to show you why it's important to check on the full two-stage model
# but I'm blaming this lack of performance on pLDA more than I'm blaming it on LASSO/Ridge

# now we do Ridge
# linear Ridge cross-validation
cv.ridge <- cv.glmnet(ppctrain.mat[, -1], ppc.train$revenue, alpha = 0)
plot(cv.ridge)
lam.ridge <- cv.ridge$lambda.min
# linear Ridge regression #1 (bestlam2)
fit.ridge <- glmnet(ppctrain.mat[, -1], ppc.train$revenue, alpha = 0, lambda = lam.ridge)
fit.ridge$beta # coefficients
# training error
pred.ridgetrain <- predict(fit.ridge, ppctrain.mat[, -1], s = lam.ridge) 
sqrt(mean((ppc.train$revenue - pred.ridgetrain)^2))
# test error
pred.ridgetest <- predict(fit.ridge, ppctest.mat[, -1], s = lam.ridge)
sqrt(mean((ppc.test$revenue - pred.ridgetest)^2))

# metrics summary
lasso.errors <- c(sqrt(mean((ppc.train$revenue - pred.lassotrain)^2)), 
                  sqrt(mean((ppc.test$revenue - pred.lassotest)^2)))
ridge.errors <- c(sqrt(mean((ppc.train$revenue - pred.ridgetrain)^2)), 
                  sqrt(mean((ppc.test$revenue - pred.ridgetest)^2)))
metrics <- cbind(lasso.errors, ridge.errors)
colnames(metrics) <- c("LASSO", "Ridge")
rownames(metrics) <- c("Train", "Test")
metrics <- as.data.frame(metrics)
metrics

# the finale: what does the plot of predicted vs. true values look like for each model?
# remember: the equivalent plot for one-stage LASSO was terrible!
# you can find that one in figs/lasso_test_comparison_plot
test.compare <- cbind(ppc.test$revenue, pred.lassotest, pred.ridgetest)
IDs <- as.numeric(rownames(test.compare))
test.compare <- cbind(IDs, test.compare)
colnames(test.compare) <- c("ID", "True", "pLDA.LASSO", "pLDA.Ridge")
# LASSO plot
ggplot(data = as.data.frame(test.compare)) + 
  geom_point(mapping = aes(x = ID, y = pLDA.LASSO, color = "pLDA+LASSO"), alpha = 0.3) +
  geom_point(mapping = aes(x = ID, y = True, color = "True"), alpha = 0.3) +
  ylab("Log Revenue") + ggtitle("PPC pLDA+LASSO Predictions vs. True Revenues") +
  scale_color_manual("Legend", breaks = c("pLDA+LASSO", "True"), values = c("red", "blue"))
# Ridge plot
ggplot(data = as.data.frame(test.compare)) + 
  geom_point(mapping = aes(x = ID, y = pLDA.Ridge, color = "pLDA+Ridge"), alpha = 0.3) +
  geom_point(mapping = aes(x = ID, y = True, color = "True"), alpha = 0.3) +
  ylab("Log Revenue") + ggtitle("PPC pLDA+Ridge Predictions vs. True Revenues") +
  scale_color_manual("Legend", breaks = c("pLDA+Ridge", "True"), values = c("red", "blue"))

# WOW these plots are awful
# there are some quick improvements we can make though
# for instance, the predicted revenue for a customer classified as unpaid SHOULD BE ZERO
# if we add that data in, it might lower the overall MSE
# in fact, let's do that

# matrix of unpaid predicted customers (UPPC's)
uppc <- test[-index.ppc, ]

# vectors of final predictions
# this is a bit tricky because we actually have three groups:
# 1. points classified as "unpaid"
# 2. points classified as "paid" but used in the training set
# 3. points classified as "paid" but used in the test set
# we will first create a vector that indicates which of the above groups each point is in
# they are numbered in the order they are listed above
unpaid.nums <- which(penlda$ypred == 1)
paid.nums <- which(penlda$ypred == 2)
paid.nums.train <- paid.nums[samp2]
paid.nums.test <- paid.nums[-samp2]
# the actual group assignment vector now
groups <- rep(0, nrow(test))
groups[unpaid.nums] <- 1
groups[paid.nums.train] <- 2
groups[paid.nums.test] <- 3
groups
# of course, paid training set data isn't regressed at all, so that can be dropped
regressed <- which(groups != 2)
length(regressed) # the TRUE number of predictions made

# final predictions matrix
finalpreds <- cbind(1:nrow(test), groups)
finalpred.lasso <- rep(0, nrow(finalpreds))
finalpred.lasso[groups == 3] <- pred.lassotest
finalpred.lasso[groups == 2] <- NA
finalpred.ridge <- rep(0, nrow(finalpreds))
finalpred.ridge[groups == 3] <- pred.ridgetest
finalpred.ridge[groups == 2] <- NA
finalpreds <- cbind(finalpreds, finalpred.lasso, finalpred.ridge)
finalpreds <- cbind(finalpreds, test$revenue)
colnames(finalpreds) <- c("ID", "Group", "pLDA.LASSO", "pLDA.Ridge", "True")
finalpreds <- as.data.frame(finalpreds)
finalpreds <- finalpreds[groups != 2, ]
rownames(finalpreds) <- 1:nrow(finalpreds)

# before we plot, let's compute the TRUE MSE
final.lasso.mse <- sqrt(mean((finalpreds$True - finalpreds$pLDA.LASSO)^2))
final.ridge.mse <- sqrt(mean((finalpreds$True - finalpreds$pLDA.Ridge)^2))
# !!!!! incredible! both MSE's are around 1.342!

# but now, let's look at the plots
# LASSO plot
ggplot(data = finalpreds) + 
  geom_point(mapping = aes(x = ID, y = pLDA.LASSO, color = "pLDA+LASSO"), alpha = 0.3) +
  geom_point(mapping = aes(x = ID, y = True, color = "True"), alpha = 0.3) +
  ylab("Log Revenue") + ggtitle("Final pLDA+LASSO Predictions vs. True Revenues") +
  scale_color_manual("Legend", breaks = c("pLDA+LASSO", "True"), values = c("red", "blue"))
# Ridge plot
ggplot(data = finalpreds) + 
  geom_point(mapping = aes(x = ID, y = pLDA.Ridge, color = "pLDA+Ridge"), alpha = 0.3) +
  geom_point(mapping = aes(x = ID, y = True, color = "True"), alpha = 0.3) +
  ylab("Log Revenue") + ggtitle("Final pLDA+Ridge Predictions vs. True Revenues") +
  scale_color_manual("Legend", breaks = c("pLDA+Ridge", "True"), values = c("red", "blue"))
# sadly, the plots are still terrible
# but most of the red points are hiding behind the big blue line at 0
# also, both Ridge and LASSO sometimes predict negative values for log revenue?!
# there's a lot of room for improvement in these models for sure
# but technically, it has a better MSE than XGBoost + Tweedie!
```


##Model 2: #Boosted Trees SMOTE + xgBoost / Splines

#Not boosted trees
The team attempted multiple classification tree models. First was a normal classification tree, then ROSE and SMOTE. These latter two models down sample the majority class (in this case, those who make no purchase) and create new minority class data points.
```{r}
# Different Classfication Tree Models to run for Step 1

# 1) Classfication Tree

library(caret)
library(tidyverse)
library(zoo)
library(DMwR)
library(rpart)

load("./output/oldcleaned.Rdata")

# Splitting the data into training and test
set.seed(100)
samp <- sample(nrow(newdat), 0.7*nrow(newdat), replace = FALSE)
train <- newdat[samp,]
test <- newdat[-samp,]

# Fitting the model
set.seed(100)
cltree <- train(is.paid ~ .,
                data = train[,-1],
                parms = list(split = "gini"),
                method = "rpart",
                trControl = trainControl(method = "cv"))


# Predicting the class labels for the test set
ct.pred <- predict(cltree, newdata = test[,-1])

# Confusion Matrix for the predicted classes (positive class is is.paid = 1)
conf <- table(ct.pred, test$is.paid); conf

# Precision (true pos/true pos + false pos)
prec <- conf[2,2]/sum(conf[2,]); prec

# Recall (true pos/true pos + false neg)
rec <- conf[2,2]/sum(conf[,2]); rec

# F1 Score
f1 <- 2*(rec*prec)/(rec+prec); f1


# 2) ROSEd classification tree

#Final ROSEd newdataset
rose_train <- ROSE(is.paid ~ ., data  = data.frame(train))$data

# Fitting the model
set.seed(100)
rose.cltree <- train(factor(is.paid) ~ .,
                data = rose_train[,-1],
                parms = list(split = "gini"),
                method = "rpart",
                trControl = trainControl(method = "cv"))


# Predicting the class labels for the test set
rose.ct.pred <- predict(rose.cltree, newdata = test[,-1])

# Confusion Matrix for the predicted classes (positive class is is.paid = 1)
rose.conf <- table(rose.ct.pred, test$is.paid); rose.conf

# Precision (true pos/true pos + false pos)
rose.prec <- rose.conf[2,2]/sum(rose.conf[2,]); rose.prec

# Recall (true pos/true pos + false neg)
rose.rec <- rose.conf[2,2]/sum(rose.conf[,2]); rose.rec

# F1 Score
rose.f1 <- 2*(rose.rec*rose.prec)/(rose.rec+rose.prec); rose.f1



# 3) SMOTEd classification tree

#Final SMOTEd newdataset
smote_train <- SMOTE(is.paid ~ ., data  = data.frame(train))

# Fitting the model
set.seed(100)
smote.cltree <- train(factor(is.paid) ~ .,
                     data = smote_train[,-1],
                     parms = list(split = "gini"),
                     method = "rpart",
                     trControl = trainControl(method = "cv"))


# Predicting the class labels for the test set
smote.ct.pred <- predict(smote.cltree, newdata = test[,-1])

# Confusion Matrix for the predicted classes (positive class is is.paid = 1)
smote.conf <- table(smote.ct.pred, test$is.paid); smote.conf

# Precision (true pos/true pos + false pos)
smote.prec <- smote.conf[2,2]/sum(smote.conf[2,]); smote.prec

# Recall (true pos/true pos + false neg)
smote.rec <- smote.conf[2,2]/sum(smote.conf[,2]); smote.rec

# F1 Score
smote.f1 <- 2*(smote.rec*smote.prec)/(smote.rec+smote.prec); smote.f1



# Summary

eval.sum <- data.frame(method = c("Normal","ROSE","SMOTE"),
                       precision = c(prec,rose.prec,smote.prec),
                       recall = c(rec,rose.rec,smote.rec),
                       f1score = c(f1,rose.f1,smote.f1))

#save(eval.sum, file = "./newdata/cltree-eval.Rdata")


```

#Boosted trees

The following applies boosting to the same base tree and oversampling techniques mentioned above (ROSE and SMOTE). 
```{r}
# Different XgBoost Models to run for Step 1

# 1) Classfication Tree


load("./data/oldcleaned.Rdata")

# Splitting the data into training and test
set.seed(100)
samp <- sample(nrow(newdat), 0.7*nrow(newdat), replace = FALSE)
train <- newdat[samp,]
test <- newdat[-samp,]

# Creating the grid of tuning parameters
grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 10,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)


# Fitting the model
set.seed(100)
xgbtree <- train(factor(is.paid) ~ .,
                data = train[,-1],
                tuneGrid = grid_default,
                method = "xgbTree",
                trControl = trainControl(method = "cv"))

# Predicting the class labels for the test set
xgb.pred <- predict(xgbtree, newdata = test[,-1])

# Confusion Matrix for the predicted classes (positive class is is.paid = 1)
conf <- table(xgb.pred, test$is.paid); conf

# Precision (true pos/true pos + false pos)
prec <- conf[2,2]/sum(conf[2,]); prec

# Recall (true pos/true pos + false neg)
rec <- conf[2,2]/sum(conf[,2]); rec

# F1 Score
f1 <- 2*(rec*prec)/(rec+prec); f1


# 2) ROSEd classification tree

#Final ROSEd dataset
rose_train <- ROSE(is.paid ~ ., data  = data.frame(train))$data

# Fitting the model
set.seed(100)
rose.xgbtree <- train(factor(is.paid) ~ .,
                 data = rose_train[,-1],
                 tuneGrid = grid_default,
                 method = "xgbTree",
                 trControl = trainControl(method = "cv"))

# Predicting the class labels for the test set
rose.xgb.pred <- predict(rose.xgbtree, newdata = test[,-1])

# Confusion Matrix for the predicted classes (positive class is is.paid = 1)
rose.conf <- table(rose.xgb.pred, test$is.paid); rose.conf

# Precision (true pos/true pos + false pos)
rose.prec <- rose.conf[2,2]/sum(rose.conf[2,]); rose.prec

# Recall (true pos/true pos + false neg)
rose.rec <- rose.conf[2,2]/sum(rose.conf[,2]); rose.rec

# F1 Score
rose.f1 <- 2*(rose.rec*rose.prec)/(rose.rec+rose.prec); rose.f1



# 3) SMOTEd classification tree

#Final SMOTEd dataset
smote_train <- SMOTE(is.paid ~ ., data  = data.frame(train))

# Fitting the model
set.seed(100)
smote.xgbtree <- train(factor(is.paid) ~ .,
                 data = smote_train[,-1],
                 tuneGrid = grid_default,
                 method = "xgbTree",
                 trControl = trainControl(method = "cv"))

# Predicting the class labels for the test set
smote.xgb.pred <- predict(smote.xgbtree, newdata = test[,-1])

# Confusion Matrix for the predicted classes (positive class is is.paid = 1)
smote.conf <- table(smote.xgb.pred, test$is.paid); smote.conf

# Precision (true pos/true pos + false pos)
smote.prec <- smote.conf[2,2]/sum(smote.conf[2,]); smote.prec

# Recall (true pos/true pos + false neg)
smote.rec <- smote.conf[2,2]/sum(smote.conf[,2]); smote.rec

# F1 Score
smote.f1 <- 2*(smote.rec*smote.prec)/(smote.rec+smote.prec); smote.f1


# Summary

xgbct.eval.sum <- data.frame(method = c("Normal","ROSE","SMOTE"),
                       precision = c(prec,rose.prec,smote.prec),
                       recall = c(rec,rose.rec,smote.rec),
                       f1score = c(f1,rose.f1,smote.f1))

#save(xgbct.eval.sum, file = "./data/xgbtree-eval.Rdata")
```
Ultimately, the team decided to use the Boosted Trees using SMOTE.



```{r}
# Two-Step Models (Merging with Step 1)
#Boosted Classification Trees (After SMOTE sampling for imbalance correction) + Xgboost (1) + MARS (2)

load("./data/oldcleaned.Rdata")

#Splitting into training and test sets
set.seed(100)
samp <- sample(nrow(newdat), 0.7*nrow(newdat), replace = FALSE)
train <- newdat[samp,]
test <- newdat[-samp,]

# SMOTE resampled training set
smote_train <- SMOTE(is.paid ~ ., data  = data.frame(train))

# Step 1: Fitting the classfication model
set.seed(100)
smote.xgbtree <- train(factor(is.paid) ~ .,
                       data = smote_train[,-1],
                       tuneGrid = grid_default,
                       method = "xgbTree",
                       trControl = trainControl(method = "cv"))

# Predicting the class labels for the test set
smote.xgb.pred <- predict(smote.xgbtree, newdata = test[,-1])

# Create a new set for the regression
newset <- cbind(test,smote.xgb.pred)

# Only care about the paid classified users
paidset <- newset[newset$smote.xgb.pred == 1,][,-c(2,16)]

#Step 2: Fitting the regression models

#Splitting into training & Test again
set.seed(100)
samp <- sample(nrow(paidset), 0.7*nrow(paidset), replace = FALSE)
training <- paidset[samp,]
valid <- paidset[-samp,]
summary(training)
summary(valid)

# Create a model matrix
mod <- model.matrix(revenue ~ ., data = training)
modte <- model.matrix(revenue ~ ., data = valid)

# The Step 2 (Model 1 - XgBoost)
xgb.final <- xgboost(data = mod, 
               label = training$revenue, 
               eta = 0.01,
               max_depth = 4, 
               nrounds = 1000,
               eval_metric = "rmse",
               objective = "reg:linear"#,
               #tweedie_variance_power = 1.1
)

#Predicted revenue 
y_pred.xgb <- predict(xgb.final, newdata = modte)

#RMSE
sqrt(mean((y_pred.xgb-valid$revenue)^2))



# Model 2 (Multivariate Adaptive Regression Splines)
mars <- earth(revenue~., data = training, degree = 3)

y_pred.mars <- predict(mars, newdata = valid)

sqrt(mean((y_pred.mars-valid$revenue)^2))
```

## Conclusion

